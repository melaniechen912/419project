{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017018,
     "end_time": "2023-04-05T22:00:52.416040",
     "exception": false,
     "start_time": "2023-04-05T22:00:52.399022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Simple PyTorch Training pipeline to train using SnapMix Augmentation.** \n",
    "\n",
    "*References :* \n",
    "* https://arxiv.org/abs/2012.04846\n",
    "* https://github.com/Shaoli-Huang/SnapMix\n",
    "\n",
    "\n",
    "* V15 : Base version\n",
    "* V17 : Added warmup epoch(1), reducing snapmix to 0.5, Moved back prop etc outside autocast(as suggested in docs)\n",
    "* V18 : Increased lr of backbone and running for 10 epochs now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:00:52.454699Z",
     "iopub.status.busy": "2023-04-05T22:00:52.453843Z",
     "iopub.status.idle": "2023-04-05T22:01:11.422082Z",
     "shell.execute_reply": "2023-04-05T22:01:11.421227Z",
     "shell.execute_reply.started": "2023-04-05T20:15:49.957717Z"
    },
    "papermill": {
     "duration": 18.989968,
     "end_time": "2023-04-05T22:01:11.422204",
     "exception": false,
     "start_time": "2023-04-05T22:00:52.432236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\r\n",
      "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 549 kB 13.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.8.1)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (5.3.1)\r\n",
      "Collecting huggingface-hub\r\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 199 kB 57.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (5.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.0.10)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.1.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.23.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.45.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.1.0)\r\n",
      "Collecting packaging>=20.9\r\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.25.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.0.4)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (0.18.2)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (0.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (1.18.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (1.18.5)\r\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (8.0.1)\r\n",
      "Collecting typing-extensions>=3.7.4.3\r\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\n",
      "Installing collected packages: typing-extensions, packaging, huggingface-hub, timm\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing-extensions 3.7.4.1\r\n",
      "    Uninstalling typing-extensions-3.7.4.1:\r\n",
      "      Successfully uninstalled typing-extensions-3.7.4.1\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 20.1\r\n",
      "    Uninstalling packaging-20.1:\r\n",
      "      Successfully uninstalled packaging-20.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bokeh 2.2.3 requires tornado>=5.1, but you have tornado 5.0.2 which is incompatible.\r\n",
      "aiobotocore 1.1.2 requires botocore<1.17.45,>=1.17.44, but you have botocore 1.19.31 which is incompatible.\u001b[0m\r\n",
      "Successfully installed huggingface-hub-0.13.3 packaging-23.0 timm-0.6.13 typing-extensions-4.5.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:11.469209Z",
     "iopub.status.busy": "2023-04-05T22:01:11.468640Z",
     "iopub.status.idle": "2023-04-05T22:01:15.088319Z",
     "shell.execute_reply": "2023-04-05T22:01:15.087603Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.375780Z"
    },
    "papermill": {
     "duration": 3.645628,
     "end_time": "2023-04-05T22:01:15.088436",
     "exception": false,
     "start_time": "2023-04-05T22:01:11.442808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import timm\n",
    "from torchvision import models as tvmodels\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.131862Z",
     "iopub.status.busy": "2023-04-05T22:01:15.130574Z",
     "iopub.status.idle": "2023-04-05T22:01:15.136216Z",
     "shell.execute_reply": "2023-04-05T22:01:15.135693Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.391604Z"
    },
    "papermill": {
     "duration": 0.028426,
     "end_time": "2023-04-05T22:01:15.136305",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.107879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from albumentations import Compose\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.179096Z",
     "iopub.status.busy": "2023-04-05T22:01:15.178407Z",
     "iopub.status.idle": "2023-04-05T22:01:15.180912Z",
     "shell.execute_reply": "2023-04-05T22:01:15.181443Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.404778Z"
    },
    "papermill": {
     "duration": 0.025884,
     "end_time": "2023-04-05T22:01:15.181564",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.155680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.224397Z",
     "iopub.status.busy": "2023-04-05T22:01:15.223782Z",
     "iopub.status.idle": "2023-04-05T22:01:15.226573Z",
     "shell.execute_reply": "2023-04-05T22:01:15.226110Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.418889Z"
    },
    "papermill": {
     "duration": 0.02609,
     "end_time": "2023-04-05T22:01:15.226666",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.200576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/cassava-leaf-disease-classification/'\n",
    "NUM_FOLDS = 5\n",
    "bs = 32\n",
    "# Running only 5 epochs to test (Train more offline ^_^)\n",
    "EPOCHS = 10\n",
    "sz = 512\n",
    "SNAPMIX_ALPHA = 5.0\n",
    "SNAPMIX_PCT = 0.5\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "TIMM_MODEL = 'resnet50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.268409Z",
     "iopub.status.busy": "2023-04-05T22:01:15.267803Z",
     "iopub.status.idle": "2023-04-05T22:01:15.270190Z",
     "shell.execute_reply": "2023-04-05T22:01:15.270736Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.432111Z"
    },
    "papermill": {
     "duration": 0.024948,
     "end_time": "2023-04-05T22:01:15.270844",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.245896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO : Play around with SNAPMIX_PCT, SNAPMIX_ALPHA and EPOCHS to converge for max accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.315544Z",
     "iopub.status.busy": "2023-04-05T22:01:15.314912Z",
     "iopub.status.idle": "2023-04-05T22:01:15.320919Z",
     "shell.execute_reply": "2023-04-05T22:01:15.320464Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.451196Z"
    },
    "papermill": {
     "duration": 0.030448,
     "end_time": "2023-04-05T22:01:15.321011",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.290563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 1234\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01917,
     "end_time": "2023-04-05T22:01:15.359231",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.340061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cassava Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.407715Z",
     "iopub.status.busy": "2023-04-05T22:01:15.406967Z",
     "iopub.status.idle": "2023-04-05T22:01:15.409306Z",
     "shell.execute_reply": "2023-04-05T22:01:15.409721Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.464561Z"
    },
    "papermill": {
     "duration": 0.031038,
     "end_time": "2023-04-05T22:01:15.409830",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.378792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    \"\"\"Cassava dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (string): dataframe train/valid\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get_img_bgr_to_rgb(self, path):\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        return im_rgb\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.dataframe.iloc[idx, 0])\n",
    "        image = self.get_img_bgr_to_rgb(img_name)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        csv_row = self.dataframe.iloc[idx, 1:]\n",
    "        sample = {\n",
    "            'image': image, \n",
    "            'label': csv_row.label,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.454110Z",
     "iopub.status.busy": "2023-04-05T22:01:15.453606Z",
     "iopub.status.idle": "2023-04-05T22:01:15.479619Z",
     "shell.execute_reply": "2023-04-05T22:01:15.479164Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.490162Z"
    },
    "papermill": {
     "duration": 0.050376,
     "end_time": "2023-04-05T22:01:15.479717",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.429341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019151,
     "end_time": "2023-04-05T22:01:15.518237",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.499086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transforms using albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.564824Z",
     "iopub.status.busy": "2023-04-05T22:01:15.564193Z",
     "iopub.status.idle": "2023-04-05T22:01:15.567238Z",
     "shell.execute_reply": "2023-04-05T22:01:15.566794Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.552328Z"
    },
    "papermill": {
     "duration": 0.029366,
     "end_time": "2023-04-05T22:01:15.567322",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.537956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_transforms():\n",
    "    return Compose([\n",
    "            A.RandomResizedCrop(sz, sz),\n",
    "            #A.Transpose(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            #A.VerticalFlip(p=0.5),\n",
    "            #A.ShiftScaleRotate(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            A.RandomBrightness(limit=0.3, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "\n",
    "def valid_transforms():\n",
    "    return Compose([\n",
    "            A.Resize(sz, sz),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            A.RandomBrightness(limit=0.3, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019056,
     "end_time": "2023-04-05T22:01:15.606279",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.587223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model (modified to support SnapMix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.653004Z",
     "iopub.status.busy": "2023-04-05T22:01:15.652332Z",
     "iopub.status.idle": "2023-04-05T22:01:15.655627Z",
     "shell.execute_reply": "2023-04-05T22:01:15.655184Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.577552Z"
    },
    "papermill": {
     "duration": 0.029881,
     "end_time": "2023-04-05T22:01:15.655720",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.625839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassavaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = timm.create_model(TIMM_MODEL, pretrained=True)\n",
    "        n_features = backbone.fc.in_features\n",
    "        self.backbone = nn.Sequential(*backbone.children())[:-2]\n",
    "        self.classifier = nn.Linear(n_features, 5)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.forward_features(x)\n",
    "        x = self.pool(feats).view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022959,
     "end_time": "2023-04-05T22:01:15.697786",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.674827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.741081Z",
     "iopub.status.busy": "2023-04-05T22:01:15.740393Z",
     "iopub.status.idle": "2023-04-05T22:01:15.742894Z",
     "shell.execute_reply": "2023-04-05T22:01:15.743283Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.605797Z"
    },
    "papermill": {
     "duration": 0.026031,
     "end_time": "2023-04-05T22:01:15.743391",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.717360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(input, targs):\n",
    "    return accuracy_score(targs.cpu(), input.cpu())\n",
    "\n",
    "def print_scores(scores):\n",
    "    kaggle_metric = np.average(scores)\n",
    "    print(\"Kaggle Metric: %f\" % (kaggle_metric))\n",
    "    \n",
    "    return kaggle_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.787218Z",
     "iopub.status.busy": "2023-04-05T22:01:15.786588Z",
     "iopub.status.idle": "2023-04-05T22:01:15.789478Z",
     "shell.execute_reply": "2023-04-05T22:01:15.789027Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.625758Z"
    },
    "papermill": {
     "duration": 0.027005,
     "end_time": "2023-04-05T22:01:15.789600",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.762595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def prec_recall_fscore(input, targs):\n",
    "    scores = precision_recall_fscore_support(targs.cpu(), input.cpu(), average='macro')\n",
    "    return {'precision': scores[0], 'recall': scores[1], 'fscore': scores[2], 'support': scores[3]} # return dictionary of 4 values\n",
    "\n",
    "def print_prec_recall_fscore(scores):\n",
    "    print(\"Precision Recall FScores Metric:\")\n",
    "    print(scores)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:15.884491Z",
     "iopub.status.busy": "2023-04-05T22:01:15.883697Z",
     "iopub.status.idle": "2023-04-05T22:01:15.886370Z",
     "shell.execute_reply": "2023-04-05T22:01:15.885953Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.642050Z"
    },
    "papermill": {
     "duration": 0.077483,
     "end_time": "2023-04-05T22:01:15.886467",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.808984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.054556,
     "end_time": "2023-04-05T22:01:15.960547",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.905991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Checkpoint method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:16.005161Z",
     "iopub.status.busy": "2023-04-05T22:01:16.004473Z",
     "iopub.status.idle": "2023-04-05T22:01:16.007486Z",
     "shell.execute_reply": "2023-04-05T22:01:16.007063Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.654059Z"
    },
    "papermill": {
     "duration": 0.027637,
     "end_time": "2023-04-05T22:01:16.007608",
     "exception": false,
     "start_time": "2023-04-05T22:01:15.979971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkpoint(model, optimizer, epoch, current_metric, best_metric, fold):\n",
    "    print(\"Metric improved from %f to %f , Saving Model at Epoch #%d\" % (best_metric, current_metric, epoch))\n",
    "    ckpt = {\n",
    "        'model': CassavaNet(),\n",
    "        'state_dict': model.state_dict(),\n",
    "        #'optimizer' : optimizer.state_dict(),  # Commenting this out to cheap out on space\n",
    "        'metric': current_metric\n",
    "    }\n",
    "    torch.save(ckpt, 'ckpt_%s-%d-%d.pth' % (TIMM_MODEL, sz, fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019361,
     "end_time": "2023-04-05T22:01:16.046417",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.027056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:16.092040Z",
     "iopub.status.busy": "2023-04-05T22:01:16.091316Z",
     "iopub.status.idle": "2023-04-05T22:01:16.094147Z",
     "shell.execute_reply": "2023-04-05T22:01:16.093737Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.672006Z"
    },
    "papermill": {
     "duration": 0.028288,
     "end_time": "2023-04-05T22:01:16.094240",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.065952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, \n",
    "                        random_state=SEED).split(np.arange(train_df.shape[0]), \n",
    "                                                 train_df.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019223,
     "end_time": "2023-04-05T22:01:16.133162",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.113939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SnapMix Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:16.196574Z",
     "iopub.status.busy": "2023-04-05T22:01:16.195900Z",
     "iopub.status.idle": "2023-04-05T22:01:16.198723Z",
     "shell.execute_reply": "2023-04-05T22:01:16.198284Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.685592Z"
    },
    "papermill": {
     "duration": 0.046471,
     "end_time": "2023-04-05T22:01:16.198815",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.152344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def get_spm(input,target,model):\n",
    "    imgsize = (sz, sz)\n",
    "    bs = input.size(0)\n",
    "    with torch.no_grad():\n",
    "        output,fms = model(input)\n",
    "        clsw = model.classifier\n",
    "        weight = clsw.weight.data\n",
    "        bias = clsw.bias.data\n",
    "        weight = weight.view(weight.size(0),weight.size(1),1,1)\n",
    "        fms = F.relu(fms)\n",
    "        poolfea = F.adaptive_avg_pool2d(fms,(1,1)).squeeze()\n",
    "        clslogit = F.softmax(clsw.forward(poolfea))\n",
    "        logitlist = []\n",
    "        for i in range(bs):\n",
    "            logitlist.append(clslogit[i,target[i]])\n",
    "        clslogit = torch.stack(logitlist)\n",
    "\n",
    "        out = F.conv2d(fms, weight, bias=bias)\n",
    "\n",
    "        outmaps = []\n",
    "        for i in range(bs):\n",
    "            evimap = out[i,target[i]]\n",
    "            outmaps.append(evimap)\n",
    "\n",
    "        outmaps = torch.stack(outmaps)\n",
    "        if imgsize is not None:\n",
    "            outmaps = outmaps.view(outmaps.size(0),1,outmaps.size(1),outmaps.size(2))\n",
    "            outmaps = F.interpolate(outmaps,imgsize,mode='bilinear',align_corners=False)\n",
    "\n",
    "        outmaps = outmaps.squeeze()\n",
    "\n",
    "        for i in range(bs):\n",
    "            outmaps[i] -= outmaps[i].min()\n",
    "            outmaps[i] /= outmaps[i].sum()\n",
    "\n",
    "\n",
    "    return outmaps,clslogit\n",
    "\n",
    "\n",
    "def snapmix(input, target, alpha, model=None):\n",
    "\n",
    "    r = np.random.rand(1)\n",
    "    lam_a = torch.ones(input.size(0))\n",
    "    lam_b = 1 - lam_a\n",
    "    target_b = target.clone()\n",
    "\n",
    "    if True:\n",
    "        wfmaps,_ = get_spm(input, target, model)\n",
    "        bs = input.size(0)\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam1 = np.random.beta(alpha, alpha)\n",
    "        rand_index = torch.randperm(bs).cuda()\n",
    "        wfmaps_b = wfmaps[rand_index,:,:]\n",
    "        target_b = target[rand_index]\n",
    "\n",
    "        same_label = target == target_b\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "        bbx1_1, bby1_1, bbx2_1, bby2_1 = rand_bbox(input.size(), lam1)\n",
    "\n",
    "        area = (bby2-bby1)*(bbx2-bbx1)\n",
    "        area1 = (bby2_1-bby1_1)*(bbx2_1-bbx1_1)\n",
    "\n",
    "        if  area1 > 0 and  area>0:\n",
    "            ncont = input[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()\n",
    "            ncont = F.interpolate(ncont, size=(bbx2-bbx1,bby2-bby1), mode='bilinear', align_corners=True)\n",
    "            input[:, :, bbx1:bbx2, bby1:bby2] = ncont\n",
    "            lam_a = 1 - wfmaps[:,bbx1:bbx2,bby1:bby2].sum(2).sum(1)/(wfmaps.sum(2).sum(1)+1e-8)\n",
    "            lam_b = wfmaps_b[:,bbx1_1:bbx2_1,bby1_1:bby2_1].sum(2).sum(1)/(wfmaps_b.sum(2).sum(1)+1e-8)\n",
    "            tmp = lam_a.clone()\n",
    "            lam_a[same_label] += lam_b[same_label]\n",
    "            lam_b[same_label] += tmp[same_label]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "            lam_a[torch.isnan(lam_a)] = lam\n",
    "            lam_b[torch.isnan(lam_b)] = 1-lam\n",
    "\n",
    "    return input,target,target_b,lam_a.cuda(),lam_b.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02078,
     "end_time": "2023-04-05T22:01:16.238892",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.218112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SnapMix Criterion (Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:16.283225Z",
     "iopub.status.busy": "2023-04-05T22:01:16.282601Z",
     "iopub.status.idle": "2023-04-05T22:01:16.285091Z",
     "shell.execute_reply": "2023-04-05T22:01:16.285501Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.792014Z"
    },
    "papermill": {
     "duration": 0.02682,
     "end_time": "2023-04-05T22:01:16.285633",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.258813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SnapMixLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, criterion, outputs, ya, yb, lam_a, lam_b):\n",
    "        loss_a = criterion(outputs, ya)\n",
    "        loss_b = criterion(outputs, yb)\n",
    "        loss = torch.mean(loss_a * lam_a + loss_b * lam_b)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019671,
     "end_time": "2023-04-05T22:01:16.324599",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.304928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ADDED: FOCAL LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:16.375479Z",
     "iopub.status.busy": "2023-04-05T22:01:16.371355Z",
     "iopub.status.idle": "2023-04-05T22:01:16.378253Z",
     "shell.execute_reply": "2023-04-05T22:01:16.377833Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.806026Z"
    },
    "papermill": {
     "duration": 0.034334,
     "end_time": "2023-04-05T22:01:16.378348",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.344014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019031,
     "end_time": "2023-04-05T22:01:16.416644",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.397613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T22:01:16.479819Z",
     "iopub.status.busy": "2023-04-05T22:01:16.473298Z",
     "iopub.status.idle": "2023-04-05T23:21:12.965759Z",
     "shell.execute_reply": "2023-04-05T23:21:12.966250Z",
     "shell.execute_reply.started": "2023-04-05T20:15:56.849628Z"
    },
    "papermill": {
     "duration": 4796.530578,
     "end_time": "2023-04-05T23:21:12.966400",
     "exception": false,
     "start_time": "2023-04-05T22:01:16.435822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1_0-14fe96d1.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_a1_0-14fe96d1.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Adjusting learning rate of group 1 to 1.0000e-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 1, 'train_loss': 1.0332}: 100%|██████████| 534/534 [06:45<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 1, 'train_loss': 1.0332, 'val_loss': 0.7142}\n",
      "Kaggle Metric: 0.747590\n",
      "Metric improved from 0.000000 to 0.747590 , Saving Model at Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'train_loss': 0.8304}: 100%|██████████| 534/534 [06:36<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'train_loss': 0.8304, 'val_loss': 0.6355}\n",
      "Kaggle Metric: 0.776819\n",
      "Metric improved from 0.747590 to 0.776819 , Saving Model at Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 3, 'train_loss': 0.8282}: 100%|██████████| 534/534 [06:38<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 3, 'train_loss': 0.8282, 'val_loss': 0.6311}\n",
      "Kaggle Metric: 0.771999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 4, 'train_loss': 0.8147}: 100%|██████████| 534/534 [06:37<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n",
      "{'Epoch': 4, 'train_loss': 0.8147, 'val_loss': 0.6036}\n",
      "Kaggle Metric: 0.784981\n",
      "Metric improved from 0.776819 to 0.784981 , Saving Model at Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 5, 'train_loss': 0.8035}: 100%|██████████| 534/534 [06:37<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 5, 'train_loss': 0.8035, 'val_loss': 0.6024}\n",
      "Kaggle Metric: 0.791433\n",
      "Metric improved from 0.784981 to 0.791433 , Saving Model at Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 6, 'train_loss': 0.7925}: 100%|██████████| 534/534 [06:37<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n",
      "{'Epoch': 6, 'train_loss': 0.7925, 'val_loss': 0.5863}\n",
      "Kaggle Metric: 0.789179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 7, 'train_loss': 0.7709}: 100%|██████████| 534/534 [06:36<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 7, 'train_loss': 0.7709, 'val_loss': 0.5827}\n",
      "Kaggle Metric: 0.792522\n",
      "Metric improved from 0.791433 to 0.792522 , Saving Model at Epoch #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 8, 'train_loss': 0.7811}: 100%|██████████| 534/534 [06:36<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 8, 'train_loss': 0.7811, 'val_loss': 0.5683}\n",
      "Kaggle Metric: 0.802006\n",
      "Metric improved from 0.792522 to 0.802006 , Saving Model at Epoch #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 9, 'train_loss': 0.7702}: 100%|██████████| 534/534 [06:36<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 9, 'train_loss': 0.7702, 'val_loss': 0.5722}\n",
      "Kaggle Metric: 0.795009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 10, 'train_loss': 0.761}: 100%|██████████| 534/534 [06:39<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 10, 'train_loss': 0.761, 'val_loss': 0.5585}\n",
      "Kaggle Metric: 0.800218\n"
     ]
    }
   ],
   "source": [
    "for fold_num, (train_split, valid_split) in enumerate(folds):\n",
    "    train_set = train_df.iloc[train_split].reset_index(drop=True)\n",
    "    valid_set = train_df.iloc[valid_split].reset_index(drop=True)\n",
    "    \n",
    "    train_ds = CassavaDataset(dataframe=train_set,\n",
    "                          root_dir=DATA_PATH + 'train_images',\n",
    "                          transforms=train_transforms())\n",
    "    \n",
    "    valid_ds = CassavaDataset(dataframe=valid_set,\n",
    "                          root_dir=DATA_PATH + 'train_images',\n",
    "                          transforms=valid_transforms())\n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=bs, \n",
    "                                           shuffle=True, num_workers=8, drop_last=True,\n",
    "                                           pin_memory=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=bs, \n",
    "                                           shuffle=False, num_workers=8, \n",
    "                                           pin_memory=True)\n",
    "    \n",
    "    losses = []\n",
    "    batches = len(train_dl)\n",
    "    val_batches = len(valid_dl)\n",
    "    best_metric = 0\n",
    "    \n",
    "    model = CassavaNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "    val_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # criterion = FocalLoss().to(device)\n",
    "    # val_criterion = FocalLoss().to(device)\n",
    "    snapmix_criterion = SnapMixLoss().to(device)\n",
    "    \n",
    "    param_groups = [\n",
    "       {'params': model.backbone.parameters(), 'lr': 1e-2},\n",
    "       {'params': model.classifier.parameters()},\n",
    "    ]\n",
    "    optimizer = torch.optim.SGD(param_groups, lr=1e-1, momentum=0.9,\n",
    "                                weight_decay=1e-4, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1,20,40], \n",
    "                                                     gamma=0.1, last_epoch=-1, verbose=True)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # ----------------- TRAINING  ----------------- \n",
    "        train_loss = 0\n",
    "        progress = tqdm(enumerate(train_dl), desc=\"Loss: \", total=batches)\n",
    "\n",
    "        model.train()\n",
    "        for i, data in progress:\n",
    "            image, label = data.values()\n",
    "            X, y = image.to(device).float(), label.to(device).long()\n",
    "            \n",
    "            with autocast():\n",
    "                \n",
    "                rand = np.random.rand()\n",
    "                if rand > (1.0-SNAPMIX_PCT):\n",
    "                    X, ya, yb, lam_a, lam_b = snapmix(X, y, SNAPMIX_ALPHA, model)\n",
    "                    outputs, _ = model(X)\n",
    "                    loss = snapmix_criterion(criterion, outputs, ya, yb, lam_a, lam_b)\n",
    "                    # loss = torch.mean(criterion(outputs, y))                        \n",
    "                else:\n",
    "                    outputs, _ = model(X)\n",
    "                    loss = torch.mean(criterion(outputs, y))                \n",
    "            scaler.scale(loss).backward()\n",
    "            # Accumulate gradients\n",
    "            if ((i + 1) % GRAD_ACCUM_STEPS == 0) or ((i + 1) == len(train_dl)):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            cur_step = i+1\n",
    "            trn_epoch_result = dict()\n",
    "            trn_epoch_result['Epoch'] = epoch + 1\n",
    "            # trn_epoch_result['FLtrain_loss'] = round(train_loss/cur_step, 4)\n",
    "            trn_epoch_result['train_loss'] = round(train_loss/cur_step, 4)\n",
    "\n",
    "            progress.set_description(str(trn_epoch_result))\n",
    "\n",
    "        scheduler.step()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # ----------------- VALIDATION  ----------------- \n",
    "        val_loss = 0\n",
    "        scores = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valid_dl):\n",
    "                image, label = data.values()\n",
    "                X, y = image.to(device), label.to(device)\n",
    "                outputs, _ = model(X)\n",
    "                l = val_criterion(outputs, y)\n",
    "                val_loss += l.item()\n",
    "\n",
    "                preds = F.softmax(outputs).argmax(axis=1)\n",
    "                scores.append(accuracy_metric(preds, y))                \n",
    "\n",
    "        epoch_result = dict()\n",
    "        epoch_result['Epoch'] = epoch + 1\n",
    "        epoch_result['train_loss'] = round(train_loss/batches, 4)\n",
    "        epoch_result['val_loss'] = round(val_loss/val_batches, 4)\n",
    "\n",
    "        print(epoch_result)\n",
    "\n",
    "        # Check if we need to save\n",
    "        current_metric = print_scores(scores)\n",
    "        # print_prec_recall_fscore(prec_recall_scores)\n",
    "        \n",
    "        if current_metric > best_metric:\n",
    "            checkpoint(model, optimizer, epoch+1, current_metric, best_metric, fold_num)\n",
    "            best_metric = current_metric\n",
    "            \n",
    "    del model, optimizer, train_dl, valid_dl, scaler, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train only a single fold\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T23:21:18.842195Z",
     "iopub.status.busy": "2023-04-05T23:21:18.841243Z",
     "iopub.status.idle": "2023-04-05T23:21:18.850739Z",
     "shell.execute_reply": "2023-04-05T23:21:18.851145Z",
     "shell.execute_reply.started": "2023-04-05T21:35:23.447655Z"
    },
    "papermill": {
     "duration": 2.796335,
     "end_time": "2023-04-05T23:21:18.851266",
     "exception": false,
     "start_time": "2023-04-05T23:21:16.054931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Recall FScores Metric:\n",
      "{'precision': 0.825, 'recall': 0.8875, 'fscore': 0.830952380952381, 'support': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.825,\n",
       " 'recall': 0.8875,\n",
       " 'fscore': 0.830952380952381,\n",
       " 'support': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_prec_recall_fscore(prec_recall_fscore(preds, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4833.519079,
   "end_time": "2023-04-05T23:21:22.185238",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-05T22:00:48.666159",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
